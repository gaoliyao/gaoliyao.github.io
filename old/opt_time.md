## On Optimal Early Stopping: Over-informative versus Under-informative Parametrization

I wish to note here the analysis was first derived by Ruoqi. It is a very remarkable way of thinking to this problem. 

Main idea of the Theorem. 
In setting $S_{over}$, for a fixed parameter $\theta^* $ and noise variance $\sigma^2$, when $n\leq d$, let $\gamma=\left(\frac{\sqrt n+\sqrt{2\log n}}{\sqrt d}\right)^2$. For $\gamma\leq 1$, with probability at least $1-\frac{2}{n}$ over the randomness of $X$, the optimal early stopping time $t_{opt}$ satisfies

$$
\frac{n}{(1+\sqrt{\gamma})^2d}\log(1+\frac{(1-\sqrt{\gamma})^2||\theta^*||^2_2}{\sigma^2}) \leq t_{opt} \leq \frac{n}{(1-\sqrt{\gamma})^2d}\log(1+\frac{(1+\sqrt{\gamma})^2||\theta^*||^2_2}{\sigma^2}).
$$

Main part of the proof. 
The risk is related to the maximum eigenvalue of the random Wischart matrix generated by $\frac{1}{n}X^TX$. Taking the derivative we could try to see points that solves the first order derivative. This maximum eigenvalue is natually bounded by the following Lemma. 

Lemma 2. 
Let $X\in\mathbb{R}^{n\times d}$, $d\geq n$ be a matrix whose entries are independent Gaussian random variables following $\mathcal{N}(0, 1)$. Let $\lambda_1 \geq ...\geq \lambda_d$ be the $d$ eigenvalues of the matrix $\frac{1}{n}X^TX$. Let $\gamma=\left(\frac{\sqrt{n}+\sqrt{2\log n}}{\sqrt{d}}\right)^2$. For $\gamma\leq 1$, $\lambda_{n+1}=...=\lambda_d=0$, and with probability at least $1-\frac{2}{n}$, 
$$
\frac{d}{n}(1-\sqrt{\gamma})^2 \leq \lambda_n \leq \lambda_1 \leq \frac{d}{n}(1+\sqrt{\gamma})^2.
$$

Note Lemma 2 here is having $d\geq n$. When compressing into $n$ eigenvalues, there are potentially more values (N(0, 1)) performing summation. If $n\geq d$ (under-informative), we could derive the classic proof similar to # Nakkiran et. al. 

### Reference
Nakkiran, Preetum, et al. "Optimal regularization can mitigate double descent." _arXiv preprint arXiv:2003.01897_ (2020).
